<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <title>Bed-Time Story Generator</title>

  <!-- 1. ONNX Runtime from CDN -->
  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort.min.js"></script>
  <script>
    // 2. Point at the CDN WASM files
    ort.env.wasm.wasmPaths = {
      "ort-wasm.wasm":               "https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort-wasm.wasm",
      "ort-wasm-simd.wasm":          "https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort-wasm-simd.wasm",
      "ort-wasm-threaded.wasm":      "https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort-wasm-threaded.wasm",
      "ort-wasm-simd-threaded.wasm": "https://cdn.jsdelivr.net/npm/onnxruntime-web/dist/ort-wasm-simd-threaded.wasm"
    };
  </script>

  <style>
    body { font-family: sans-serif; padding: 2em; }
    textarea { width: 100%; box-sizing: border-box; }
    button { margin-top: .5em; padding: .5em 1em; }
    pre { background: #f4f4f4; padding: 1em; white-space: pre-wrap; }
  </style>
</head>
<body>
  <h1>Bed-Time Story Generator</h1>
  <textarea id="prompt" rows="4" placeholder="Once upon a time…"></textarea><br>
  <button id="generate">Tell me a story!</button>
  <pre id="story"></pre>

  <script>
    // 3. Load the “with-past” ONNX model
    const sessionPromise = ort.InferenceSession.create(
      "./models/TinyStories-Instruct-1M/TinyStories-Instruct-1M.onnx",
      { executionProviders: ["wasm"] }
    );

    async function generateStory(promptText, maxNewTokens = 64) {
      const session = await sessionPromise;
      const encoder = new TextEncoder();
      const decoder = new TextDecoder();

      // 4. Tokenize the prompt
      const promptBytes = encoder.encode(promptText);
      let inputIds       = Array.from(promptBytes);
      let attentionMask  = Array(inputIds.length).fill(1);

      // 5. Build position_ids for the full prompt
      let positionIds      = inputIds.map((_, i) => i);
      let currentPosition  = positionIds.length;

      // 6. Prepare empty cache feeds ([1,16,4,0] float32) based on the model’s config:
      //    hidden_size=64, num_heads=16 → head_dim=4 :contentReference[oaicite:0]{index=0}
      const cacheInputNames = session.inputNames.filter(n =>
        n.startsWith("past_key_values.")
      );
      const pastFeeds = {};
      for (let name of cacheInputNames) {
        pastFeeds[name] = new ort.Tensor(
          "float32",
          new Float32Array(0),
          [1, 16, 4, 0]
        );
      }

      const generated = [];

      // 7. Autoregressive loop
      for (let step = 0; step < maxNewTokens; step++) {
        // a) Assemble feeds: input_ids, attention_mask, position_ids, + past cache
        const feeds = {
          input_ids:      new ort.Tensor("int32", Int32Array.from(inputIds),       [1, inputIds.length]),
          attention_mask: new ort.Tensor("int32", Int32Array.from(attentionMask),  [1, attentionMask.length]),
          position_ids:   new ort.Tensor("int32", Int32Array.from(positionIds),    [1, positionIds.length]),
          ...pastFeeds
        };

        // b) Run one step
        const outputs = await session.run(feeds);

        // c) Greedy decode: pick highest-logit token
        const logits = outputs.logits.data;
        let maxLogit = -Infinity, nextId = 0;
        for (let i = 0; i < logits.length; i++) {
          if (logits[i] > maxLogit) {
            maxLogit = logits[i];
            nextId = i;
          }
        }
        generated.push(nextId);

        // d) Prepare for next step: single-token input & mask
        inputIds      = [ nextId ];
        attentionMask = [ 1 ];

        // e) Next position_id = the next absolute position
        positionIds      = [ currentPosition ];
        currentPosition += 1;

        // f) Swap in the new cache (“present” → “past”)
        for (let name in outputs) {
          if (name.startsWith("present_key_values.")) {
            const pastName = name.replace("present_key_values", "past_key_values");
            pastFeeds[pastName] = outputs[name];
          }
        }
      }

      // 8. Decode and return just the continuation text
      return decoder.decode(new Uint8Array(generated));
    }

    document.getElementById("generate").onclick = async () => {
      const prompt = document.getElementById("prompt").value.trim() || "Once upon a time";
      document.getElementById("story").textContent = "⏳ Generating…";
      const continuation = await generateStory(prompt, 128);
      document.getElementById("story").textContent = prompt + continuation;
    };
  </script>
</body>
</html>
