<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width,initial-scale=1">
  <title>Bedtime Story Generator</title>
  <style>
    body { font-family: Arial; max-width:800px; margin:20px auto; background:#f0f4f8; }
    h1 { text-align:center; color:#333; }
    #prompt {
      width:100%; padding:10px; font-size:16px;
      border:1px solid #ccc; border-radius:5px;
    }
    .button-container {
      display:flex; gap:10px; margin:20px 0;
    }
    button {
      flex:1; padding:10px; border:none;
      border-radius:5px; color:white; cursor:pointer;
    }
    #random-btn { background:#008CBA; }
    button:not(#random-btn) { background:#4CAF50; }
    button:hover { opacity:0.9; }
    #story {
      background:white; padding:20px; border:1px solid #ddd;
      border-radius:5px; min-height:100px;
    }
    #loading { display:none; text-align:center; color:#666; }
  </style>
</head>
<body>
  <h1>Bedtime Story Generator</h1>
  <input id="prompt" placeholder="Enter a story prompt…" />
  <div class="button-container">
    <button onclick="generateStory()">Generate Story</button>
    <button id="random-btn" onclick="generateRandomStory()">Random Story</button>
  </div>
  <div id="loading">Generating story…</div>
  <div id="story"></div>

  <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.18.0/dist/ort.min.js"></script>
  <script>
    // ──────────────── CONFIG ────────────────
    const CACHE_NUM_HEADS = 16;   // your model’s # attention heads
    const CACHE_HEAD_SIZE = 4;    // head_dim = hidden_size/heads = 64/16
    const MAX_NEW_TOKENS  = 50;   // how many tokens to generate
    const EOS_TOKEN_ID    = 2;    // adjust to your model’s EOS

    let session = null;           // ONNX Session
    let vocab   = null;           // id→token map

    // ───────── LOAD VOCAB JSON ─────────
    async function loadVocab() {
      const r = await fetch('./models/TinyStories-Instruct-1M/vocab.json');
      vocab = await r.json();
      console.log('Loaded vocab, size=', Object.keys(vocab).length);
    }

    // ───────── INIT ONNX MODEL ──────────
    async function initModel() {
      session = await ort.InferenceSession.create(
        './models/TinyStories-Instruct-1M/TinyStories-Instruct-1M.onnx',
        { executionProviders: ['wasm'] }
      );
      console.log('Model inputs:', session.inputNames);
      console.log('Model outputs:', session.outputNames);
    }

    // ───────── TOKENIZER PLACEHOLDER ────
    // You should replace this with a real tokenizer that matches your ONNX export!
    function tokenize(prompt) {
      const words = prompt.split(' ');
      const ids = words.map((_,i)=>i+1);
      return new ort.Tensor(
        'int64',
        BigInt64Array.from(ids.map(BigInt)),
        [1, ids.length]
      );
    }

    // ───────── BUTTON HOOKS ─────────────
    async function generateStory() {
      const p = document.getElementById('prompt').value.trim();
      if (!p) { alert('Enter a prompt or click Random Story.'); return; }
      await runInference(p);
    }
    async function generateRandomStory() {
      const pool = [
        'A curious fox in a glowing forest',
        'A tiny dragon who lost its fire',
        'A bunny exploring a starry meadow',
        'A magical treehouse adventure',
        'A friendly ghost in a cozy village'
      ];
      const p = pool[Math.floor(Math.random()*pool.length)];
      document.getElementById('prompt').value = p;
      await runInference(p);
    }

    // ────────── RUN + DECODE LOOP ─────────
    async function runInference(prompt) {
      // UI prep
      document.getElementById('story').innerHTML = '';
      document.getElementById('loading').style.display = 'block';

      // lazy‐load model + vocab
      if (!session) await initModel();
      if (!vocab)   await loadVocab();

      // 1) initial tokens from prompt
      const inputTensor = tokenize(prompt);
      let tokenIds = Array.from(inputTensor.data).map(x => Number(x));
      
      // 2) build zero‐cache for all past_key_values.*
      const metaArray = Array.isArray(session.inputMetadata)
        ? session.inputMetadata : [];
      const metaMap = metaArray.reduce((m,v)=>{ m[v.name]=v; return m; }, {});
      let past = {};
      for (let name of session.inputNames) {
        if (name.startsWith('past_key_values')) {
          const md = metaMap[name];
          const dims = md && Array.isArray(md.shape)
            ? md.shape.map(d=>typeof d==='number'?d:0)
            : [1, CACHE_NUM_HEADS, 0, CACHE_HEAD_SIZE];
          const size = dims.reduce((a,b)=>a*b,1);
          past[name] = new ort.Tensor(
            'float32',
            new Float32Array(size),
            dims
          );
        }
      }

      // 3) generation loop
      for (let step = 0; step < MAX_NEW_TOKENS; step++) {
        const lastId = tokenIds[tokenIds.length - 1];

        // build per‐step inputs
        const inputIds = new ort.Tensor(
          'int64',
          BigInt64Array.from([BigInt(lastId)]),
          [1,1]
        );
        // attention over entire sequence
        const attnMask = new ort.Tensor(
          'int64',
          BigInt64Array.from(tokenIds.map(BigInt)),
          [1, tokenIds.length]
        );
        // positions = 0..seqLen-1
        const posIds = new ort.Tensor(
          'int64',
          BigInt64Array.from(tokenIds.map((_,i)=>BigInt(i))),
          [1, tokenIds.length]
        );

        const feeds = {
          input_ids:      inputIds,
          attention_mask: attnMask,
          position_ids:   posIds,
          ...past
        };

        // run the model forward
        const outputMap = await session.run(feeds);

        // extract logits (replace 'logits' if yours differs)
        const logits = outputMap.logits ?? outputMap[session.outputNames[0]];
        // shape [1,1,vocabSize]
        const arr = logits.data;
        // greedy argmax
        let nextId = 0, best = -Infinity;
        for (let i = 0; i < arr.length; i++) {
          if (arr[i] > best) { best = arr[i]; nextId = i; }
        }
        tokenIds.push(nextId);

        // collect new past_key_values for next step
        past = {};
        for (let name of session.outputNames) {
          if (name.startsWith('past_key_values')) {
            past[name] = outputMap[name];
          }
        }

        // stop if we hit EOS
        if (nextId === EOS_TOKEN_ID) break;
      }

      // 4) detokenize via vocab.json
      let text = '';
      for (let id of tokenIds) {
        const tok = vocab[id] || '<unk>';
        // many HF tokenizers use 'Ġ' as a “space” marker:
        text += tok.startsWith('Ġ') ? ' ' + tok.slice(1) : tok;
      }

      // display
      document.getElementById('loading').style.display = 'none';
      document.getElementById('story').innerHTML = `<p>${text.trim()}</p>`;
    }

    // kick things off
    initModel();
    loadVocab();
  </script>
</body>
</html>
